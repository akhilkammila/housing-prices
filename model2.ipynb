{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and Other Submissions\n",
    "Eda.ipynb and model.ipynb were my quick first model (score around 0.135)\n",
    "- No feature engineering\n",
    "- Sqrt of SalePrice\n",
    "- Ordinal encoding for all categorical variables\n",
    "- Grid search on XGBoost (single model)\n",
    "\n",
    "This model2 is now inspired by reading some other Kaggle submissions\n",
    "\n",
    "[Top 1% Solution w/ Data Leakage](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/discussion/83751)\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "1. Fill NA values (some data leakage here with using medians)\n",
    "2. Fill NA numerics w/ 0\n",
    "3. Transform numeric features with box-cox (data leakage here as well)\n",
    "4. Summed and year features (ex. bathrooms = \\sum(bathrooms))\n",
    "5. Has features (has pool, has 2nd floor, etc.)\n",
    "\n",
    "Additional Engineering\n",
    "\n",
    "6. Add dummies for categorical variables\n",
    "7. Remove columns where one value dominates\n",
    "8. Remove outliers\n",
    "\n",
    "Modelling\n",
    "\n",
    "1. Ridge Regression (Robust Scalar)\n",
    "2. Lasso Regression\n",
    "3. Elastic Net Regression\n",
    "4. Support Vector Regression\n",
    "5. Gradient Boosting Regressor\n",
    "6. Light GBM\n",
    "7. XGBoost\n",
    "8. StackingCVRegressor w/ XGBoost as meta regressor\n",
    "\n",
    "Final prediction is Blend of all previous models (including Stacking Regressor as one of those models)\n",
    "\n",
    "[Top 4% no Data Leakage](https://www.kaggle.com/code/miftahuladib/housing-price-regression-top-4?scriptVersionId=202452540)\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "1. Sum features (like baths, porcharea, rooms)\n",
    "2. Year features (ex. transform to 2025 - yearbuilt)\n",
    "3. Fill NA Values (simply fill with 0 or 'No')\n",
    "\n",
    "Additional Engineering\n",
    "\n",
    "1. Remove Outliers (based on scatterplots of various numerical features)\n",
    "2. Column Transformer (numeric --> standardScalar, ordinal --> ordinalEncoder, categorical --> oneHotEncoded)\n",
    "\n",
    "Modelling\n",
    "\n",
    "1. Random forest regressor\n",
    "2. XGBoost\n",
    "3. Ridge regression\n",
    "4. Light GBM\n",
    "5. CatBoost\n",
    "6. VotingRegressor (not used)\n",
    "7. StackingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals and Plan\n",
    "\n",
    "Goal: Understand which components of the other submissions are most relevant (how they affect score)\n",
    "\n",
    "These components include:\n",
    "\n",
    "1. Feature Engineering (which new features are best: summed features, year features, has features)\n",
    "2. Feature Transformation and Filling (boxcox transformation, fill na with null vs. values)\n",
    "3. Data leakage (how much does it help)\n",
    "\n",
    "4. Column Transformers (scaling, ordinal transform vs. one hot encoding)\n",
    "5. Removing outliers\n",
    "\n",
    "6. Model blending\n",
    "\n",
    "Base model:\n",
    "1. All possible engineered features\n",
    "2. No boxcox transformations, fill na with 0/'No'\n",
    "3. No scaling, all ordinal transform\n",
    "4. No outlier removal\n",
    "5. XGBoost only\n",
    "\n",
    "Unilaterally change the following and Record Test Score:\n",
    "1. Removing engineered features\n",
    "2. Boxcox transformation (with and without data leakage)\n",
    "3. Scaling + Separate ordinal and One Hot Transformations\n",
    "4. Outlier removal\n",
    "5. Models blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, RobustScaler, PowerTransformer, FunctionTransformer\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer,root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.csv'\n",
    "df = pd.read_csv(train_path)\n",
    "X, Y = df.drop(labels=['SalePrice'], axis=1), np.log1p(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "def fillNull(X):\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in numeric_dtypes: X[col] = X[col].fillna(0)\n",
    "        else: X[col] = X[col].fillna('No')\n",
    "\n",
    "fillNull(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (simply taken from other Kaggle submissions, goal is to see effectiveness)\n",
    "\n",
    "def featureEngineering(X: pd.DataFrame):\n",
    "    # Summed features\n",
    "    X['YrBltAndRemod']=X['YearBuilt']+X['YearRemodAdd']\n",
    "    X['TotalSF']=X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "    X['Total_sqr_footage'] = (X['BsmtFinSF1'] + X['BsmtFinSF2'] +\n",
    "                                    X['1stFlrSF'] + X['2ndFlrSF'])\n",
    "    X['Total_Bathrooms'] = (X['FullBath'] + (0.5 * X['HalfBath']) +\n",
    "                                X['BsmtFullBath'] + (0.5 * X['BsmtHalfBath']))\n",
    "    X['Total_porch_sf'] = (X['OpenPorchSF'] + X['3SsnPorch'] +\n",
    "                                X['EnclosedPorch'] + X['ScreenPorch'] +\n",
    "                                X['WoodDeckSF'])\n",
    "\n",
    "    # Has features\n",
    "    X['haspool'] = X['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    X['has2ndfloor'] = X['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    X['hasgarage'] = X['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    X['hasbsmt'] = X['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    X['hasfireplace'] = X['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "featureEngineering(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model(modelTuple, params):\n",
    "    categorical_cols = [col for col in X.columns if X[col].dtype not in numeric_dtypes]\n",
    "    numerical_cols = [col for col in X.columns if X[col].dtype in numeric_dtypes]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('encoder', ColumnTransformer([\n",
    "            ('scalar', RobustScaler(), numerical_cols),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ], remainder='passthrough')),\n",
    "        \n",
    "        modelTuple\n",
    "    ])\n",
    "\n",
    "    gridsearch = GridSearchCV(\n",
    "        pipeline,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        param_grid=params,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    gridsearch.fit(X, Y)\n",
    "\n",
    "    results = pd.DataFrame(gridsearch.cv_results_['params'])\n",
    "    results['train_rmse'] = gridsearch.cv_results_['mean_train_score']\n",
    "    results['test_rmse'] = gridsearch.cv_results_['mean_test_score']\n",
    "    results = results.sort_values(by='test_rmse', ascending=False)\n",
    "    display(results.head())\n",
    "\n",
    "    return gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb__learning_rate</th>\n",
       "      <th>xgb__max_depth</th>\n",
       "      <th>xgb__n_estimators</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>-0.070775</td>\n",
       "      <td>-0.123162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>-0.052016</td>\n",
       "      <td>-0.123283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.100</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.124919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>-0.124968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.079954</td>\n",
       "      <td>-0.125945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgb__learning_rate  xgb__max_depth  xgb__n_estimators  train_rmse  \\\n",
       "2               0.005               3               3000   -0.070775   \n",
       "5               0.010               3               3000   -0.052016   \n",
       "8               0.100               3               3000   -0.001816   \n",
       "7               0.100               3                500   -0.035962   \n",
       "6               0.100               3                100   -0.079954   \n",
       "\n",
       "   test_rmse  \n",
       "2  -0.123162  \n",
       "5  -0.123283  \n",
       "8  -0.124919  \n",
       "7  -0.124968  \n",
       "6  -0.125945  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XGB\n",
    "xgb_tuple = ('xgb', xgb.XGBRegressor(random_state=42))\n",
    "\n",
    "params_xgb = {\n",
    "    'xgb__n_estimators': [100,500,3000],\n",
    "    'xgb__learning_rate': [0.005, 0.01, 0.1],\n",
    "    'xgb__max_depth': [3]\n",
    "    }\n",
    "\n",
    "xgb = find_model(xgb_tuple, params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge__alpha</th>\n",
       "      <th>ridge__solver</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>sag</td>\n",
       "      <td>-0.112776</td>\n",
       "      <td>-0.138088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>sparse_cg</td>\n",
       "      <td>-0.112776</td>\n",
       "      <td>-0.138088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>auto</td>\n",
       "      <td>-0.112776</td>\n",
       "      <td>-0.138088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>sag</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.138090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>sparse_cg</td>\n",
       "      <td>-0.112445</td>\n",
       "      <td>-0.138090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ridge__alpha ridge__solver  train_rmse  test_rmse\n",
       "19            19           sag   -0.112776  -0.138088\n",
       "18            19     sparse_cg   -0.112776  -0.138088\n",
       "16            19          auto   -0.112776  -0.138088\n",
       "15            18           sag   -0.112445  -0.138090\n",
       "14            18     sparse_cg   -0.112445  -0.138090"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridge_tuple = ('ridge', Ridge())\n",
    "\n",
    "params_ridge = {\n",
    "    'ridge__alpha': [15,16,17,18,19],\n",
    "    'ridge__solver': ['auto', 'lsqr', 'sparse_cg', 'sag']\n",
    "}\n",
    "\n",
    "ridge = find_model(ridge_tuple, params_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:658: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.12951792479109, tolerance: 0.018810611883705176\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:658: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.198517373085981, tolerance: 0.019008081403702633\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:658: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.105321117080706, tolerance: 0.018912592760396085\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:658: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.286781494562189, tolerance: 0.01800219138548883\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Users/akhilkammila/miniconda3/envs/tradingClub/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:658: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.225656594894114, tolerance: 0.018373605848561597\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso__alpha</th>\n",
       "      <th>lasso__max_iter</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>100000</td>\n",
       "      <td>-0.115823</td>\n",
       "      <td>-0.136192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>100000</td>\n",
       "      <td>-0.090461</td>\n",
       "      <td>-0.153008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>100000</td>\n",
       "      <td>-0.147881</td>\n",
       "      <td>-0.156266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020</td>\n",
       "      <td>100000</td>\n",
       "      <td>-0.159842</td>\n",
       "      <td>-0.167734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lasso__alpha  lasso__max_iter  train_rmse  test_rmse\n",
       "3         0.001           100000   -0.115823  -0.136192\n",
       "0         0.000           100000   -0.090461  -0.153008\n",
       "1         0.010           100000   -0.147881  -0.156266\n",
       "2         0.020           100000   -0.159842  -0.167734"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "lasso_tuple = ('lasso', Lasso())\n",
    "\n",
    "params_lasso = {\n",
    "    'lasso__alpha': [0,0.01, 0.02, 0.001],\n",
    "    'lasso__max_iter': [100000]\n",
    "}\n",
    "\n",
    "lasso = find_model(lasso_tuple, params_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13095986328730336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.13418579739329983"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.1342932779460285"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_model(model):\n",
    "    test_path = './data/test.csv'\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    fillNull(test_df)\n",
    "    featureEngineering(test_df)\n",
    "\n",
    "    test_y = model.predict(test_df)\n",
    "\n",
    "    test_df['SalePrice'] = np.expm1(test_y)\n",
    "    test_df[['Id', 'SalePrice']].to_csv('results.csv', index=False)\n",
    "\n",
    "    solution_path = './data/solution.csv'\n",
    "    solution_df = pd.read_csv(solution_path)\n",
    "    score = root_mean_squared_error(np.log(test_df['SalePrice']), np.log(solution_df['SalePrice']))\n",
    "    return score\n",
    "\n",
    "display(test_model(xgb))\n",
    "display(test_model(ridge))\n",
    "display(test_model(lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Feature Engineering:\n",
    "1. No feature engineering: 0.1345\n",
    "2. Summed features: 0.1307\n",
    "3. Has features: 0.1345\n",
    "\n",
    "Scaling:\n",
    "1. Baseline ordinal encoding:  0.1307\n",
    "2. One hot encoding (instead of ordinal): 0.1317\n",
    "3. One hot + Robust scalar: 0.1309\n",
    "\n",
    "Models:\n",
    "1. XGB: 0.1309\n",
    "2. Ridge regression: 0.134\n",
    "3. Lasso regression: 0.134\n",
    "\n",
    "Transformation:\n",
    "1. Boxcox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradingClub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
