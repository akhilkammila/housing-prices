{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and Other Submissions\n",
    "Eda.ipynb and model.ipynb were my quick first model (score around 0.135)\n",
    "- No feature engineering\n",
    "- Sqrt of SalePrice\n",
    "- Ordinal encoding for all categorical variables\n",
    "- Grid search on XGBoost (single model)\n",
    "\n",
    "This model2 is now inspired by reading some other Kaggle submissions\n",
    "\n",
    "[Top 1% Solution w/ Data Leakage](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/discussion/83751)\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "1. Fill NA values (some data leakage here with using medians)\n",
    "2. Fill NA numerics w/ 0\n",
    "3. Transform numeric features with box-cox (data leakage here as well)\n",
    "4. Summed and year features (ex. bathrooms = \\sum(bathrooms))\n",
    "5. Has features (has pool, has 2nd floor, etc.)\n",
    "\n",
    "Additional Engineering\n",
    "\n",
    "6. Add dummies for categorical variables\n",
    "7. Remove columns where one value dominates\n",
    "8. Remove outliers\n",
    "\n",
    "Modelling\n",
    "\n",
    "1. Ridge Regression (Robust Scalar)\n",
    "2. Lasso Regression\n",
    "3. Elastic Net Regression\n",
    "4. Support Vector Regression\n",
    "5. Gradient Boosting Regressor\n",
    "6. Light GBM\n",
    "7. XGBoost\n",
    "8. StackingCVRegressor w/ XGBoost as meta regressor\n",
    "\n",
    "Final prediction is Blend of all previous models (including Stacking Regressor as one of those models)\n",
    "\n",
    "[Top 4% no Data Leakage](https://www.kaggle.com/code/miftahuladib/housing-price-regression-top-4?scriptVersionId=202452540)\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "1. Sum features (like baths, porcharea, rooms)\n",
    "2. Year features (ex. transform to 2025 - yearbuilt)\n",
    "3. Fill NA Values (simply fill with 0 or 'No')\n",
    "\n",
    "Additional Engineering\n",
    "\n",
    "1. Remove Outliers (based on scatterplots of various numerical features)\n",
    "2. Column Transformer (numeric --> standardScalar, ordinal --> ordinalEncoder, categorical --> oneHotEncoded)\n",
    "\n",
    "Modelling\n",
    "\n",
    "1. Random forest regressor\n",
    "2. XGBoost\n",
    "3. Ridge regression\n",
    "4. Light GBM\n",
    "5. CatBoost\n",
    "6. VotingRegressor (not used)\n",
    "7. StackingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals and Plan\n",
    "\n",
    "Goal: Understand which components of the other submissions are most relevant (how they affect score)\n",
    "\n",
    "These components include:\n",
    "\n",
    "1. Feature Engineering (which new features are best: summed features, year features, has features)\n",
    "2. Feature Transformation and Filling (boxcox transformation, fill na with null vs. values)\n",
    "3. Data leakage (how much does it help)\n",
    "\n",
    "4. Column Transformers (scaling, ordinal transform vs. one hot encoding)\n",
    "5. Removing outliers\n",
    "\n",
    "6. Model blending\n",
    "\n",
    "Base model:\n",
    "1. All possible engineered features\n",
    "2. No boxcox transformations, fill na with 0/'No'\n",
    "3. No scaling, all ordinal transform\n",
    "4. No outlier removal\n",
    "5. XGBoost only\n",
    "\n",
    "Unilaterally change the following and Record Test Score:\n",
    "1. Removing engineered features\n",
    "2. Boxcox transformation (with and without data leakage)\n",
    "3. Scaling + Separate ordinal and One Hot Transformations\n",
    "4. Outlier removal\n",
    "5. Models blended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
